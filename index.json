[{"authors":null,"categories":null,"content":"I am a Ph.D. student in reinforcement learning for combinatorial optimization at Inria/CNRS in the SequeL/ScooL team, under the supervision of P. Preux. My research interests also include graph representation learning and geometric deep learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://nathan-grinsztajn.netlify.app/author/nathan-grinsztajn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nathan-grinsztajn/","section":"authors","summary":"I am a Ph.D. student in reinforcement learning for combinatorial optimization at Inria/CNRS in the SequeL/ScooL team, under the supervision of P. Preux. My research interests also include graph representation learning and geometric deep learning.","tags":null,"title":"Nathan Grinsztajn","type":"authors"},{"authors":null,"categories":null,"content":"Supervisors: Philippe Preux, Nathan Grinsztajn\nDuration: 5 to 6 months\nWhen: Spring-Summer 2021\nWhere: Scool (previously SequeL), Inria Lille, Villeneuve d\u0026rsquo;Ascq, France\nExpected background: master in CS, specialized in machine learning.\nKeywords: reinforcement learning, combinatorial optimization, experimental.\nContext: Reinforcement learning is a sub-field of machine learning in which we aim at designing agents that learn to act. Acting usually involves performing a sequence of actions in order to achieve a goal. Examples are countless; games are good examples, like Pacman or chess in which the player has to perform a series of actions either to reach a maximal score, or to defeat his opponent. Applications of RL go way beyond games.\nRecently, there were trials to use RL to solve hard combinatorial problems. Some major examples (and successes) are GO, Rubiks Cube, Scheduling, Maximum Independent Set (MIS), Maximum Coverage (MC), Minimum Vertex Cover (MVC)\u0026hellip; Graph coloring, however useful in practice (bandwidth allocation, scheduling\u0026hellip;) and difficult, received less attention. State of the art graph coloring solvers still mainly rely on tabu search (tabucol), and hand-crafted heuristics. It is to be expected that hybriding RL and tabu search will provide more efficient algorithms.\nWhat: The goal of this internship is to:\n Study the literature of this problem. This includes deep reinforcement learning, graph neural networks, graph coloring, tabu search. Perform an experimental assessment of the ideas. Explore new ideas on combining RL and tabu search (or other search algorithms). This exploration can be theoretical or algorithmic.  Bibliography:\n Sutton, Barto, Reinforcement Learning, an Introduction, 2nd edition, 2018. Battaglia et al. Relational Inductive Biases, Deep Learning, and Graph Networks ArXiv:1806.01261, October 17, 2018. A. Hertz, D. de Werra, Using tabu search techniques for graph coloring. Computing 39, 345–351 (1987).  Working environment: Scool (previously SequeL) is a well-known research group in reinforcement learning and bandits. It is composed of 5 permanent researchers, 20+ PhD students, a couple of post-docs and engineers. Scool provides a very rich and stimulating for doing cutting-edge research in RL.\n","date":1605826800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605826800,"objectID":"6732ebb52b9059fd605dff6edf50ba4b","permalink":"https://nathan-grinsztajn.netlify.app/internships/tabucol/","publishdate":"2020-11-20T00:00:00+01:00","relpermalink":"/internships/tabucol/","section":"internships","summary":"Learning to solve graph coloring problems and exploring ideas combining RL and tabu search for combinatorial optimization.","tags":null,"title":"RL and Tabu Search for Graph Coloring Problems","type":"internships"},{"authors":null,"categories":null,"content":"Supervisors: Philippe Preux, Nathan Grinsztajn\nDuration: 5 to 6 months\nWhen: Spring-Summer 2021\nWhere: Scool (previously SequeL), Inria Lille, Villeneuve d\u0026rsquo;Ascq, France\nExpected background: master in CS, specialized in machine learning.\nKeywords: reinforcement learning, combinatorial optimization, experimental.\nContext: Combinatorial Optimization Problems (COP) constitute an important family of fundamental problems: traveling salesman, vehicle routing problem, stable marriage , graph coloring, task scheduling, and many others. There are various algorithmic approaches, ranging from (provably) exact methods (e.g. based on tree search, linear programming\u0026hellip;) to non (provably) exact/approximate methods (heuristics and meta-heuristics). Those methods are able to solve large scale COPs, but they require a careful investigation of the problem.\nOn the other hand, real world applications bring another set of challenges: inherent uncertainty in the definition of the problem and randomness in the process dynamics. Task Graph Scheduling consists in mapping a task graph onto a target platform: nodes denote computational tasks, and edges model precedence constraints between tasks. When the full graph is unknown at the beginning of the scheduling (e.g. because of user interactions), known heuristics fall short. We propose to study and implement a RL approach to solve this kind of task.\nWhat: The goal of this internship is to:\n Study the literature of this problem. This includes deep reinforcement learning, graph neural networks, task graph scheduling. Perform an experimental assessment of these ideas. Explore new ideas on combining RL and dynamic graphs (e.g. the Canadian Traveller Problem (CTP)). This exploration can be theoretical or algorithmic.  Bibliography:\n Sutton, Barto, Reinforcement Learning, an Introduction, 2nd edition, 2018 Aditya Paliwal et al, Reinforced genetic algorithm learning for optimizing computation graphs. InProc. ICLR, page 24,2020 Battaglia et al. Relational Inductive Biases, Deep Learning, and Graph Networks ArXiv:1806.01261, October 17, 2018. Grinsztajn et al, Geometric Deep Reinforcement Learning for Dynamic DAG Scheduling. Advances in IEEE ADPRL (ADPRL 2020).  Working environment: Scool (previously SequeL) is a well-known research group in reinforcement learning and bandits. It is composed of 5 permanent researchers, 20+ PhD students, a couple of post-docs and engineers. Scool provides a very rich and stimulating for doing cutting-edge research in RL.\n","date":1605826800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605826800,"objectID":"b0a0e9e0b13fe3ff6b6182c9f857dc14","permalink":"https://nathan-grinsztajn.netlify.app/internships/hpc/","publishdate":"2020-11-20T00:00:00+01:00","relpermalink":"/internships/hpc/","section":"internships","summary":"Task Graph Scheduling is the activity that consists in mapping a task graph onto a target platform: nodes denote computational tasks, and edges model precedence constraints between tasks. When the full graph is unknown at the beginning of the scheduling (e.g. because of user interactions), known heuristics fall short. We propose to study and implement a RL approach to solve this kind of task.","tags":null,"title":"RL for Dynamic Task Graph Scheduling","type":"internships"},{"authors":["Nathan Grinsztajn","Daniel Furelos-Blanco","Thomas D. Barrett"],"categories":null,"content":"","date":1665141060,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665141060,"objectID":"739fffc822a790638c841cc77dcd27d8","permalink":"https://nathan-grinsztajn.netlify.app/publication/poppy/","publishdate":"2022-10-07T12:11:00+01:00","relpermalink":"/publication/poppy/","section":"publication","summary":"Applying reinforcement learning (RL) to combinatorial optimization problems is attractive as it removes the need for expert knowledge or pre-solved instances. However, it is unrealistic to expect an agent to solve these (often NP-)hard problems in a single shot at inference due to their inherent complexity. Thus, leading approaches often implement additional search strategies, from stochastic sampling and beam-search to explicit fine-tuning. In this paper, we argue for the benefits of learning a population of complementary policies, which can be simultaneously rolled out at inference. To this end, we introduce Poppy, a simple theoretically grounded training procedure for populations. Instead of relying on a predefined or hand-crafted notion of diversity, Poppy induces an unsupervised specialization targeted solely at maximizing the performance of the population. We show that Poppy produces a set of complementary policies, and obtains state-of-the-art RL results on three popular NP-hard problems: the traveling salesman (TSP), the capacitated vehicle routing (CVRP), and 0-1 knapsack (KP) problems. On TSP specifically, Poppy outperforms the previous state-of-the-art, dividing the optimality gap by 5 while reducing the inference time by more than an order of magnitude.","tags":[],"title":"Population-Based Reinforcement Learning for Combinatorial Optimization","type":"publication"},{"authors":["Nathan Grinsztajn","Toby Johnstone","Johan Ferret","Philippe Preux"],"categories":null,"content":"","date":1634728260,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634728260,"objectID":"d7e0d81c03066eefee512289c1f63e1c","permalink":"https://nathan-grinsztajn.netlify.app/publication/easee/","publishdate":"2021-10-20T12:11:00+01:00","relpermalink":"/publication/easee/","section":"publication","summary":"Incorporating prior knowledge in reinforcement learning algorithms is mainly an open question. Even when insights about the environment dynamics are available, reinforcement learning is traditionally used in a tabula rasa setting and must explore and learn everything from scratch.\nIn this paper, we consider the problem of exploiting priors about action sequence equivalence: that is, when different sequences of actions produce the same effect.\nWe propose a new local exploration strategy calibrated to minimize collisions and maximize new state visitations. We show that this strategy can be computed at little cost, by solving a convex optimization problem.\nBy replacing the usual epsilon-greedy strategy in a DQN, we demonstrate its potential in several environments with various dynamic structures.","tags":[],"title":"More Efficient Exploration with Symbolic Priors on Action Sequence Equivalences","type":"publication"},{"authors":["Manh Hung Nguyen","Nathan Grinsztajn","Lisheng Sun-Hosoya","Isabelle Guyon"],"categories":null,"content":"","date":1631531460,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631531460,"objectID":"43dc44aa107bad985304b62119d64501","permalink":"https://nathan-grinsztajn.netlify.app/publication/meta_rl/","publishdate":"2021-09-13T12:11:00+01:00","relpermalink":"/publication/meta_rl/","section":"publication","summary":"This paper addresses a cornerstone of Automated Machine Learning: the problem of rapidly uncovering which machine learning algorithm performs best on a new dataset. Our approach leverages performances of such algorithms on datasets to which they have been previously exposed, ie, implementing a form of meta-learning. More specifically, the problem is cast as a REVEAL Reinforcement Learning (RL) game: the meta-learning problem is wrapped into a RL environment in which an agent can start, pause, or resume training various machine learning algorithms to progressively “reveal” their learning curves. The learned policy is then applied to quickly uncover the best algorithm on a new dataset. While other similar approaches, such as Freeze-Thaw, were proposed in the past, using Bayesian optimization, our methodology is, to the best of our knowledge, the first that trains a RL agent to do this task on previous datasets. Using real and artificial data, we show that our new RL-based meta-learning paradigm outperforms Free-Thaw and other baseline methods, with respect to the Area under the Learning curve metric, a form of evaluation of Any-time learning (ie, the capability of interrupting the algorithm at any time while obtaining good performance).","tags":[],"title":"MetaREVEAL: RL-based Meta-learning from Learning Curves","type":"publication"},{"authors":["Nathan Grinsztajn","Johan Ferret","Olivier Pietquin","Philippe Preux","Matthieu Geist"],"categories":null,"content":"","date":1631099520,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631099520,"objectID":"54608212f9dde9f710c099bebf74764a","permalink":"https://nathan-grinsztajn.netlify.app/publication/reversibility/","publishdate":"2021-09-08T12:12:00+01:00","relpermalink":"/publication/reversibility/","section":"publication","summary":"We propose to learn to distinguish reversible from irreversible actions for better informed decision-making in Reinforcement Learning (RL). From theoretical considerations, we show that approximate reversibility can be learned through a simple surrogate task: ranking randomly sampled trajectory events in chronological order. Intuitively, pairs of events that are always observed in the same order are likely to be separated by an irreversible sequence of actions. Conveniently, learning the temporal order of events can be done in a fully self-supervised way, which we use to estimate the reversibility of actions from experience, without any priors. We propose two different strategies that incorporate reversibility in RL agents, one strategy for exploration (RAE) and one strategy for control (RAC). We demonstrate the potential of reversibility-aware agents in several environments, including the challenging Sokoban game. In synthetic tasks, we show that we can learn control policies that never fail and reduce to zero the side-effects of interactions, even without access to the reward function.","tags":[],"title":"There Is No Turning Back: A Self-Supervised Approach for Reversibility-Aware Reinforcement Learning","type":"publication"},{"authors":["Nathan Grinsztajn","Olivier Beaumont","Emmanuel Jeannot","Philippe Preux"],"categories":null,"content":"","date":1630494720,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630494720,"objectID":"0f595637bf62f09107b90510e57737c5","permalink":"https://nathan-grinsztajn.netlify.app/publication/readys/","publishdate":"2021-09-01T12:12:00+01:00","relpermalink":"/publication/readys/","section":"publication","summary":"In this paper, we propose READYS, a reinforcement learning algorithm for the dynamic scheduling of computations modeled as a Directed Acyclic Graph (DAGs). Our goal is to develop a scheduling algorithm in which allocation and scheduling decisions are made at runtime, based on the state of the system, as performed in runtime systems such as StarPU or ParSEC. Reinforcement Learning is a natural candidate to achieve this task, since its general principle is to build step by step a strategy that, given the state of the system (the state of the resources and a view of the ready tasks and their successors in our case), makes a decision to optimize a global criterion. Moreover, the use of Reinforcement Learning is natural in a context where the duration of tasks (and communications) is stochastic. We propose READYS that combines Graph Convolutional Networks (GCN) with an Actor-Critic Algorithm (A2C): it builds an adaptive representation of the scheduling problem on the fly and learns a scheduling strategy, aiming at minimizing the makespan. A crucial point is that READYS builds a general scheduling strategy which is neither limited to only one specific application or task graph nor one particular problem size, and that can be used to schedule any DAG. We focus on different types of task graphs originating from linear algebra factorization kernels (CHOLESKY, LU, QR) and we consider heterogeneous platforms made of a few CPUs and GPUs. We first propose to analyze the performance of READYS when learning is performed on a given (platform, kernel, problem size) combination. Using simulations, we show that the scheduling agent obtains performances very similar or even superior to algorithms from the literature, and that it is especially powerful when the scheduling environment contains a lot of uncertainty. We additionally demonstrate that our agent exhibits very promising generalization capabilities. To the best of our knowledge, this is the first paper which shows that reinforcement learning can really be used for dynamic DAG scheduling on heterogeneous resources.","tags":[],"title":"READYS: A Reinforcement Learning Based Strategy for Heterogeneous Dynamic Scheduling","type":"publication"},{"authors":["Nathan Grinsztajn","Louis Leconte","Philippe Preux","Edouard Oyallon"],"categories":null,"content":"","date":1622805120,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622805120,"objectID":"a6284c5caf770c225d8b01d1e6868317","permalink":"https://nathan-grinsztajn.netlify.app/publication/igt/","publishdate":"2021-06-04T12:12:00+01:00","relpermalink":"/publication/igt/","section":"publication","summary":"We present a new approach for learning unsupervised node representations in community graphs. We significantly extend the Interferometric Graph Transform (IGT) to community labeling: this non-linear operator iteratively extracts features that take advantage of the graph topology through demodulation operations. An unsupervised feature extraction step cascades modulus non-linearity with linear operators that aim at building relevant invariants for community labeling. Via a simplified model, we show that the IGT concentrates around the E-IGT: those two representations are related through some ergodicity properties. Experiments on community labeling tasks show that this unsupervised representation achieves performances at the level of the state of the art on the standard and challenging datasets Cora, Citeseer, Pubmed and WikiCS.","tags":[],"title":"Interferometric Graph Transform for Community Labeling","type":"publication"},{"authors":["Nathan Grinsztajn","Philippe Preux","Edouard Oyallon"],"categories":null,"content":"","date":1619867520,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619867520,"objectID":"50f73c49af3bf7fa91489982c6fa3794","permalink":"https://nathan-grinsztajn.netlify.app/publication/low_rank_projection/","publishdate":"2021-05-01T12:12:00+01:00","relpermalink":"/publication/low_rank_projection/","section":"publication","summary":"In this work, we study the behavior of standard models for community detection under spectral manipulations. Through various ablation experiments, we evaluate the impact of bandpass filtering on the performance of a GCN: we empirically show that most of the necessary and used information for nodes classification is contained in the low-frequency domain, and thus contrary to images, high frequencies are less crucial to community detection. In particular, it is sometimes possible to obtain accuracies at a state-of-the-art level with simple classifiers that rely only on a few low frequencies.","tags":[],"title":"Low-Rank Projections of GCNs Laplacian","type":"publication"},{"authors":["Nathan Grinsztajn","Olivier Beaumont","Emmanuel Jeannot","Philippe Preux"],"categories":null,"content":"","date":1603365120,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603365120,"objectID":"9fbf800c5db8fbe802a28720ab54f213","permalink":"https://nathan-grinsztajn.netlify.app/publication/hpcadprl/","publishdate":"2020-10-22T12:12:00+01:00","relpermalink":"/publication/hpcadprl/","section":"publication","summary":"In practice, it is quite common to face combinatorial optimization problems which contain uncertainty along with non-determinism and dynamicity. These three properties call for appropriate algorithms; reinforcement learning (RL) is dealing with them in a very natural way. Today, despite some efforts, most real-life combinatorial optimization problems remain out of the reach of reinforcement learning algorithms. In this paper, we propose a reinforcement learning approach to solve a realistic scheduling problem, and apply it to an algorithm commonly executed in the high performance computing community, the Cholesky factorization. On the contrary to static scheduling, where tasks are assigned to processors in a predetermined ordering before the beginning of the parallel execution, our method is dynamic: task allocations and their execution ordering are decided at runtime, based on the system state and unexpected events, which allows much more flexibility. To do so, our algorithm uses graph neural networks in combination with an actor-critic algorithm (A2C) to build an adaptive representation of the problem on the fly. We show that this approach is competitive with state-of-the-art heuristics used in high-performance computing runtime systems. Moreover, our algorithm does not require an explicit model of the environment, but we demonstrate that extra knowledge can easily be incorporated and improves performance. We also exhibit key properties provided by this RL approach, and study its transfer abilities to other instances.","tags":[],"title":"Geometric Deep Reinforcement Learning for Dynamic DAG Scheduling","type":"publication"}]